{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dir = Path('../la2a')\n",
    "train_ds_dir = ds_dir / 'Train'\n",
    "valid_ds_dir = ds_dir / 'Val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'dataset_dir': \"/mnt/d/ML/SignalTrain_LA2A_Dataset_1.1/SignalTrain_LA2A_Dataset_1.1\",\n",
    "    'train_data_len_sec': 10,\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 1e-3,\n",
    "    'num_workers': 10, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_params(filename):\n",
    "    return float(filename.split(\"__\")[1].replace(\".wav\",\"\")) / 100.0, float(filename.split(\"__\")[2].replace(\".wav\",\"\")) / 100.0 \n",
    "\n",
    "class LA2A_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset_dir, cut_sec=8, mode=\"train\"):\n",
    "        self.dataset_dir = Path(dataset_dir)\n",
    "        self.mode = mode\n",
    "\n",
    "        if mode == 'train':\n",
    "            mode_dir = self.dataset_dir / 'Train'\n",
    "        elif mode == 'valid':\n",
    "            mode_dir = self.dataset_dir / 'Val'\n",
    "        elif mode == 'test':\n",
    "            mode_dir = self.dataset_dir / 'Test' \n",
    "        else:\n",
    "            raise RuntimeError(\"Only train, valid and test modes are avalible for dataset\")\n",
    "        \n",
    "        self.input_files = list(mode_dir.glob(\"input_*.wav\"))\n",
    "        self.output_files = list(mode_dir.glob(\"target_*.wav\"))\n",
    "        self.input_files.sort()\n",
    "        self.output_files.sort()\n",
    "\n",
    "        self.params = [extract_params(str(filename)) for filename in self.output_files]\n",
    "\n",
    "        self.input_audios = []\n",
    "        self.output_audios = []\n",
    "        self.params_list = []\n",
    "        \n",
    "        for idx, (ouput_file, input_file, parameters) in tqdm(enumerate(zip(self.output_files, self.input_files, self.params)), total=len(self.input_files)):\n",
    "            inp_id = int(str(input_file.stem).split(\"_\")[1])\n",
    "            oup_id = int(str(ouput_file.stem).split(\"_\")[1])\n",
    "            assert inp_id == oup_id\n",
    "\n",
    "            input_audio, sr = torchaudio.load(input_file)\n",
    "            output_audio, sr = torchaudio.load(ouput_file)\n",
    "            assert input_audio.size()[-1] == output_audio.size()[-1]\n",
    "            assert sr == 44100\n",
    "\n",
    "            self.file_examples = []\n",
    "            \n",
    "            cut_samples = cut_sec * sr\n",
    "            padding_to_end = cut_samples - (input_audio.size()[-1] % cut_samples)\n",
    "            \n",
    "            input_audio = F.pad(input_audio, (0, padding_to_end), \"constant\", 0)\n",
    "            output_audio = F.pad(output_audio, (0, padding_to_end), \"constant\", 0)\n",
    "\n",
    "            input_chunks = input_audio.view(-1, cut_samples)\n",
    "            output_chunks = output_audio.view(-1, cut_samples)\n",
    "\n",
    "            params_chunks = torch.Tensor(parameters)[None, :]\n",
    "            params_chunks = params_chunks.repeat(input_chunks.size()[0],1)\n",
    "\n",
    "            self.input_audios.append(input_chunks)\n",
    "            self.output_audios.append(output_chunks)\n",
    "            self.params_list.append(params_chunks)\n",
    "\n",
    "        self.input_audios = torch.cat(self.input_audios, dim=0)\n",
    "        self.output_audios = torch.cat(self.output_audios, dim=0)\n",
    "        self.params_list = torch.cat(self.params_list, dim=0)\n",
    "\n",
    "        for i in tqdm(range(self.input_audios.size()[0])):\n",
    "            to_save = torch.Tensor([self.input_audios[i].cpu().numpy(), self.output_audios[i].cpu().numpy()])\n",
    "            if mode == 'train':\n",
    "                torch.save(to_save, train_ds_dir / f'{i}_{self.params_list[i].cpu().numpy()[0]:.2f}_{self.params_list[i].cpu().numpy()[1]:.2f}.pt')\n",
    "            elif mode == 'valid':\n",
    "                torch.save(to_save, valid_ds_dir / f'{i}_{self.params_list[i].cpu().numpy()[0]:.2f}_{self.params_list[i].cpu().numpy()[1]:.2f}.pt')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_audios)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_audios[idx], self.output_audios[idx], self.params_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 66/66 [15:52<00:00, 14.43s/it]\n",
      "  0%|          | 0/7076 [00:00<?, ?it/s]/tmp/ipykernel_1532/2656179095.py:62: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525551200/work/torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  to_save = torch.Tensor([self.input_audios[i].cpu().numpy(), self.output_audios[i].cpu().numpy()])\n",
      "100%|██████████| 7076/7076 [06:59<00:00, 16.85it/s]\n",
      "100%|██████████| 15/15 [02:16<00:00,  9.08s/it]\n",
      "100%|██████████| 1484/1484 [01:01<00:00, 24.13it/s]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "train_dataset = LA2A_Dataset(config['dataset_dir'],  config['train_data_len_sec'],  mode=\"train\")\n",
    "val_dataset = LA2A_Dataset(config['dataset_dir'],  config['train_data_len_sec'],  mode=\"valid\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
